{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2753993e-1f4a-48de-8f52-997a3db12828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Concatenate, BatchNormalization, ReLU, LeakyReLU, Add\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54347455-f8e2-4ea4-b40d-a11b64c26641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 08:27:09.928820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22283 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Define image dimensions\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 4096\n",
    "CHANNELS = 1\n",
    "\n",
    "# Paths to the directories containing images\n",
    "lhsa_kanjur_path = '/Resized_Selected_LG'\n",
    "computer_font_path = '/Resized_Selected_Comp'\n",
    "\n",
    "def load_images_from_folder(folder, img_height, img_width, channels):\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        img = load_img(os.path.join(folder, filename), color_mode='grayscale', target_size=(img_height, img_width))\n",
    "        if img is not None:\n",
    "            img_array = img_to_array(img)\n",
    "            images.append(img_array)\n",
    "    images = np.array(images).astype('float32')\n",
    "    images = (images - 127.5) / 127.5  # Normalize images to [-1, 1]\n",
    "    return images\n",
    "\n",
    "# Load images\n",
    "lhsa_kanjur_images = load_images_from_folder(lhsa_kanjur_path, IMG_HEIGHT, IMG_WIDTH, CHANNELS)\n",
    "computer_font_images = load_images_from_folder(computer_font_path, IMG_HEIGHT, IMG_WIDTH, CHANNELS)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_lhsa_kanjur, val_lhsa_kanjur, train_computer_font, val_computer_font = train_test_split(\n",
    "    lhsa_kanjur_images, computer_font_images, test_size=0.2, random_state=42)\n",
    "\n",
    "def create_dataset(input_images, target_images, batch_size=1):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_images, target_images))\n",
    "    dataset = dataset.shuffle(buffer_size=len(input_images))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "batch_size = 1\n",
    "train_dataset = create_dataset(train_computer_font, train_lhsa_kanjur, batch_size)\n",
    "val_dataset = create_dataset(val_computer_font, val_lhsa_kanjur, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac59b0a-71f4-46de-b5ad-342e583a7798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block\n",
    "def residual_block(x, filters, kernel_size=(3, 3)):\n",
    "    r = Conv2D(filters, kernel_size, padding='same')(x)\n",
    "    r = BatchNormalization()(r)\n",
    "    r = ReLU()(r)\n",
    "    r = Conv2D(filters, kernel_size, padding='same')(r)\n",
    "    r = BatchNormalization()(r)\n",
    "    r = Add()([r, x])\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96eb4be-16de-4383-b547-fb4c9357039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator with reduced latent dimension to 4x64\n",
    "def build_generator(input_shape):\n",
    "    inputs = Input(input_shape)  # (256, 4096, 1)\n",
    "    # Encoder\n",
    "    e1 = Conv2D(64, (4, 4), strides=2, padding='same')(inputs)  # (128, 2048, 64)\n",
    "    e2 = LeakyReLU(alpha=0.2)(BatchNormalization()(Conv2D(128, (4, 4), strides=2, padding='same')(e1)))  # (64, 1024, 128)\n",
    "    e3 = LeakyReLU(alpha=0.2)(BatchNormalization()(Conv2D(256, (4, 4), strides=2, padding='same')(e2)))  # (32, 512, 256)\n",
    "    e4 = LeakyReLU(alpha=0.2)(BatchNormalization()(Conv2D(512, (4, 4), strides=2, padding='same')(e3)))  # (16, 256, 512)\n",
    "    e5 = LeakyReLU(alpha=0.2)(BatchNormalization()(Conv2D(1024, (4, 4), strides=2, padding='same')(e4)))  # (8, 128, 1024)\n",
    "    e6 = LeakyReLU(alpha=0.2)(BatchNormalization()(Conv2D(2048, (4, 4), strides=2, padding='same')(e5)))  # (4, 64, 2048)\n",
    "    \n",
    "    # Adding Residual Blocks to the Latent Space\n",
    "    r = residual_block(e6, 2048)  # (4, 64, 2048)\n",
    "    r = residual_block(r, 2048)  # (4, 64, 2048)\n",
    "    \n",
    "    # Decoder with additional layers and skip connections\n",
    "    d1 = ReLU()(BatchNormalization()(Conv2DTranspose(1024, (4, 4), strides=2, padding='same')(r)))  # (8, 128, 1024)\n",
    "    d1 = Concatenate()([d1, e5])  # (8, 128, 2048)\n",
    "    d1 = residual_block(d1, 2048)  # (8, 128, 2048)\n",
    "    \n",
    "    d2 = ReLU()(BatchNormalization()(Conv2DTranspose(512, (4, 4), strides=2, padding='same')(d1)))  # (16, 256, 512)\n",
    "    d2 = Concatenate()([d2, e4])  # (16, 256, 1024)\n",
    "    d2 = residual_block(d2, 1024)  # (16, 256, 1024)\n",
    "    \n",
    "    d3 = ReLU()(BatchNormalization()(Conv2DTranspose(256, (4, 4), strides=2, padding='same')(d2)))  # (32, 512, 256)\n",
    "    d3 = Concatenate()([d3, e3])  # (32, 512, 512)\n",
    "    d3 = residual_block(d3, 512)  # (32, 512, 512)\n",
    "    \n",
    "    d4 = ReLU()(BatchNormalization()(Conv2DTranspose(128, (4, 4), strides=2, padding='same')(d3)))  # (64, 1024, 128)\n",
    "    d4 = Concatenate()([d4, e2])  # (64, 1024, 256)\n",
    "    d4 = residual_block(d4, 256)  # (64, 1024, 256)\n",
    "    \n",
    "    d5 = ReLU()(BatchNormalization()(Conv2DTranspose(64, (4, 4), strides=2, padding='same')(d4)))  # (128, 2048, 64)\n",
    "    d5 = Concatenate()([d5, e1])  # (128, 2048, 128)\n",
    "    d5 = residual_block(d5, 128)  # (128, 2048, 128)\n",
    "    \n",
    "    d6 = Conv2DTranspose(1, (4, 4), strides=2, padding='same', activation='tanh')(d5)  # (256, 4096, 1)\n",
    "    \n",
    "    return Model(inputs, d6)\n",
    "\n",
    "\n",
    "# Discriminator with added complexity\n",
    "def build_discriminator(input_shape):\n",
    "    inputs = Input(input_shape)  # (256, 4096, 1)\n",
    "    target = Input(input_shape)  # (256, 4096, 1)\n",
    "    combined = Concatenate()([inputs, target])  # (256, 4096, 2)\n",
    "    \n",
    "    d1 = LeakyReLU(alpha=0.2)(Conv2D(64, (4, 4), strides=2, padding='same')(combined))  # (128, 2048, 64)\n",
    "    d2 = LeakyReLU(alpha=0.2)(BatchNormalization()(Conv2D(128, (4, 4), strides=2, padding='same')(d1)))  # (64, 1024, 128)\n",
    "    d3 = LeakyReLU(alpha=0.2)(BatchNormalization()(Conv2D(256, (4, 4), strides=2, padding='same')(d2)))  # (32, 512, 256)\n",
    "    d4 = LeakyReLU(alpha=0.2)(BatchNormalization()(Conv2D(512, (4, 4), strides=1, padding='same')(d3)))  # (32, 512, 512)\n",
    "    d5 = Conv2D(1, (4, 4), strides=1, padding='same')(d4)  # (32, 512, 1)\n",
    "    \n",
    "    return Model([inputs, target], d5)\n",
    "\n",
    "# Input shape (256, 4096, 1)\n",
    "input_shape = (256, 4096, 1)\n",
    "generator = build_generator(input_shape)\n",
    "discriminator = build_discriminator(input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda83041-25c6-419e-89b4-4785d4d1481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizers\n",
    "generator_optimizer = Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "# Define the losses\n",
    "mse = MeanSquaredError()\n",
    "mae = MeanAbsoluteError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1521d1d1-aec2-498c-a12c-538748c4aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2Pix(Model):\n",
    "    def __init__(self, generator, discriminator, lambda_value=10):\n",
    "        super(Pix2Pix, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.lambda_value = lambda_value\n",
    "        self.generator_loss_tracker = tf.keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.discriminator_loss_tracker = tf.keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    def compile(self, generator_optimizer, discriminator_optimizer, gen_loss_fn, disc_loss_fn, **kwargs):\n",
    "        super(Pix2Pix, self).compile(**kwargs)\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "        self.gen_loss_fn = gen_loss_fn\n",
    "        self.disc_loss_fn = disc_loss_fn\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.generator_loss_tracker, self.discriminator_loss_tracker]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        input_image, target = data\n",
    "\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            fake_image = self.generator(input_image, training=True)\n",
    "\n",
    "            disc_real_output = self.discriminator([input_image, target], training=True)\n",
    "            disc_fake_output = self.discriminator([input_image, fake_image], training=True)\n",
    "\n",
    "            gen_gan_loss = self.gen_loss_fn(tf.ones_like(disc_fake_output), disc_fake_output)\n",
    "            gen_l1_loss = tf.reduce_mean(tf.abs(target - fake_image))\n",
    "            gen_total_loss = gen_gan_loss + (self.lambda_value * gen_l1_loss)\n",
    "\n",
    "            disc_real_loss = self.disc_loss_fn(tf.ones_like(disc_real_output), disc_real_output)\n",
    "            disc_fake_loss = self.disc_loss_fn(tf.zeros_like(disc_fake_output), disc_fake_output)\n",
    "            disc_total_loss = disc_real_loss + disc_fake_loss\n",
    "\n",
    "        generator_gradients = gen_tape.gradient(gen_total_loss, self.generator.trainable_variables)\n",
    "        discriminator_gradients = disc_tape.gradient(disc_total_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "        self.generator_optimizer.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))\n",
    "        self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.discriminator.trainable_variables))\n",
    "\n",
    "        self.generator_loss_tracker.update_state(gen_total_loss)\n",
    "        self.discriminator_loss_tracker.update_state(disc_total_loss)\n",
    "\n",
    "        return {\"generator_loss\": self.generator_loss_tracker.result(), \"discriminator_loss\": self.discriminator_loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        input_image, target = data\n",
    "        fake_image = self.generator(input_image, training=False)\n",
    "\n",
    "        disc_real_output = self.discriminator([input_image, target], training=False)\n",
    "        disc_fake_output = self.discriminator([input_image, fake_image], training=False)\n",
    "\n",
    "        gen_gan_loss = self.gen_loss_fn(tf.ones_like(disc_fake_output), disc_fake_output)\n",
    "        gen_l1_loss = tf.reduce_mean(tf.abs(target - fake_image))\n",
    "        gen_total_loss = gen_gan_loss + (self.lambda_value * gen_l1_loss)\n",
    "\n",
    "        disc_real_loss = self.disc_loss_fn(tf.ones_like(disc_real_output), disc_real_output)\n",
    "        disc_fake_loss = self.disc_loss_fn(tf.zeros_like(disc_fake_output), disc_fake_output)\n",
    "        disc_total_loss = disc_real_loss + disc_fake_loss\n",
    "\n",
    "        return {\"generator_loss\": gen_total_loss, \"discriminator_loss\": disc_total_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae3cd7d-764b-425a-8498-0d5410d219e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 143s 175ms/step - generator_loss: 5.4080 - discriminator_loss: 0.0415 - val_generator_loss: 0.0000e+00 - val_discriminator_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 140s 175ms/step - generator_loss: 5.3146 - discriminator_loss: 0.0291 - val_generator_loss: 0.0000e+00 - val_discriminator_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 140s 174ms/step - generator_loss: 5.2600 - discriminator_loss: 0.0305 - val_generator_loss: 0.0000e+00 - val_discriminator_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 139s 174ms/step - generator_loss: 5.0032 - discriminator_loss: 0.0108 - val_generator_loss: 0.0000e+00 - val_discriminator_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 139s 174ms/step - generator_loss: 4.8430 - discriminator_loss: 0.0070 - val_generator_loss: 0.0000e+00 - val_discriminator_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 139s 174ms/step - generator_loss: 4.8422 - discriminator_loss: 0.0046 - val_generator_loss: 0.0000e+00 - val_discriminator_loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 139s 174ms/step - generator_loss: 4.6828 - discriminator_loss: 0.0045 - val_generator_loss: 0.0000e+00 - val_discriminator_loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 139s 174ms/step - generator_loss: 4.6192 - discriminator_loss: 0.0043 - val_generator_loss: 0.0000e+00 - val_discriminator_loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 139s 174ms/step - generator_loss: 4.5935 - discriminator_loss: 0.0036 - val_generator_loss: 0.0000e+00 - val_discriminator_loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 139s 174ms/step - generator_loss: 4.6265 - discriminator_loss: 0.0113 - val_generator_loss: 0.0000e+00 - val_discriminator_loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 139s 174ms/step - generator_loss: 5.1146 - discriminator_loss: 0.0542 - val_generator_loss: 0.0000e+00 - val_discriminator_loss: 0.0000e+00\n",
      "Epoch 12/100\n",
      "288/800 [=========>....................] - ETA: 1:24 - generator_loss: 5.2523 - discriminator_loss: 0.0192"
     ]
    }
   ],
   "source": [
    "# Instantiate the Pix2Pix model\n",
    "#10+5+17+30\n",
    "pix2pix_model = Pix2Pix(generator, discriminator)\n",
    "\n",
    "# Compile the model\n",
    "pix2pix_model.compile(\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    discriminator_optimizer=discriminator_optimizer,\n",
    "    gen_loss_fn=mse,\n",
    "    disc_loss_fn=mse\n",
    ")\n",
    "\n",
    "# Set epochs\n",
    "EPOCHS = 100\n",
    "\n",
    "# Train the model\n",
    "history = pix2pix_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6258d8-7e55-4e98-88f1-0db9bf819ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23477878-9c67-4ba2-b33a-93272bd0db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_and_display_images(model, test_input, target, index):\n",
    "    prediction = model(test_input, training=False)\n",
    "    plt.figure(figsize=(15, 5))  # Increase the figure size to ensure high quality and images on separate lines\n",
    "    \n",
    "    display_list = [test_input[0], target[0], prediction[0]]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "    \n",
    "    for i in range(3):\n",
    "        plt.subplot(3, 1, i + 1)  # Change the layout to 3 rows, 1 column\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5, cmap='gray')  # Rescale to [0, 1] for visualization\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_and_visualize(model, val_dataset, num_images=1):  # Set default to 1 to print only one set of images\n",
    "    for i, (input_image, target) in enumerate(val_dataset.take(num_images)):\n",
    "        generate_and_display_images(model.generator, input_image, target, i)\n",
    "\n",
    "# Visualize the results on examples from the validation dataset\n",
    "evaluate_and_visualize(pix2pix_model, val_dataset, num_images=1)  # Set to 1 to print only one set of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb9d6d-f34f-4841-89f1-2e0d129e77d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
