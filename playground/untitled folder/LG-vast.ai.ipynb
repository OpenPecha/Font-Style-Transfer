{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58690f65-2cda-4b07-8f64-68db012c71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, Model\n",
    "import logging\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02add41e-bd75-4475-bc28-9185c4bf8b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization\n",
    "image_folder = \"/local_dir/Train_Images\"\n",
    "hf_dataset_name = \"ta4tsering/Lhasa_kanjur_transcription_datasets\"\n",
    "hf_dataset = datasets.load_dataset(hf_dataset_name, split=\"train\")\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"L\")  # Convert to black and white\n",
    "    image = image.resize((1400, 70))\n",
    "    image = np.array(image) / 255.0  # Normalize\n",
    "    return image\n",
    "def transcription_to_vector(transcription):\n",
    "    tokens = tokenizer(transcription, return_tensors=\"tf\", padding=\"max_length\", max_length=512, truncation=True)\n",
    "    inputs = {\n",
    "        \"input_ids\": tokens[\"input_ids\"],\n",
    "        \"attention_mask\": tokens[\"attention_mask\"]\n",
    "    }\n",
    "    outputs = model(**inputs)\n",
    "    vector = outputs.pooler_output  # Extract the pooled output as the vector representation\n",
    "    return vector\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openpecha/tibetan_RoBERTa_S_e6\")\n",
    "model = TFAutoModel.from_pretrained(\"openpecha/tibetan_RoBERTa_S_e6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c069f-cffa-42f5-9e45-cea45b36f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image preprocessing\n",
    "hf_filenames = [data[\"filename\"] for data in hf_dataset]\n",
    "print(\"Total unique filenames in Hugging Face dataset:\", len(hf_filenames))\n",
    "batch_size = 10000\n",
    "batches = []\n",
    "for i in range(0, len(hf_filenames), batch_size):\n",
    "    batch = hf_filenames[i:i + batch_size]\n",
    "    batches.append(batch)\n",
    "for i, batch in enumerate(batches):\n",
    "    print(f\"Batch {i + 1}: {len(batch)} filenames\")\n",
    "first_batch_filenames = batches[3]\n",
    "images = []\n",
    "pbar = tqdm(first_batch_filenames, desc=\"Loading and preprocessing images\", unit=\"image\")\n",
    "for filename in pbar:\n",
    "    if not filename.startswith(\".\"):  # Exclude hidden files like .DS_Store\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        image = preprocess_image(image_path)\n",
    "        h = {\"label\": filename, \"image\": image}\n",
    "        images.append(h)\n",
    "    pbar.set_postfix({'Processed': len(images)})\n",
    "dataset = Dataset.from_list(images)\n",
    "print(\"done\")\n",
    "dataset.push_to_hub(\"norbujam/LG-images-4\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52d92a-33e1-4c6e-bd68-77787aa45acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector preprocessing\n",
    "batch_vectors = []\n",
    "hf_filenames = [data[\"filename\"] for data in hf_dataset]\n",
    "hf_labels = [data[\"label\"] for data in hf_dataset]\n",
    "for i in tqdm(range(0,10000)):\n",
    "    batch_vectors.append({\"filename\": hf_filenames[i], \"vector\": transcription_to_vector(hf_labels[i])})\n",
    "dataset = Dataset.from_list(batch_vectors)\n",
    "print(\"done\")\n",
    "dataset.push_to_hub(\"norbujam/LG-vectors-1\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c857010-24a5-449c-b49f-a5dc3da835bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#common entries\n",
    "dataset_images = load_dataset(\"norbujam/LG-images-1\")\n",
    "dataset_vectors = load_dataset(\"norbujam/LG-vectors-1\")\n",
    "filenames_vectors = set(data[\"filename\"] for data in dataset_vectors[\"train\"])\n",
    "labels_images = set(data[\"label\"] for data in dataset_images[\"train\"])\n",
    "common_entries = set()\n",
    "with tqdm(total=len(labels_images), desc=\"Finding common entries\") as pbar:\n",
    "    for label in labels_images:\n",
    "        if label in filenames_vectors:\n",
    "            common_entries.add(label)\n",
    "        pbar.update(1)\n",
    "num_common_entries = len(common_entries)\n",
    "print(f\"Number of common entries between the two datasets: {num_common_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c8479a-59d5-4b81-bb5a-a47fcea96817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset upload\n",
    "dataset_images = load_dataset(\"norbujam/LG-images-4\")\n",
    "dataset_vectors = load_dataset(\"norbujam/LG-vectors-4\")\n",
    "images = [data[\"image\"] for data in tqdm(dataset_images[\"train\"])]\n",
    "vectors = [data[\"vector\"] for data in tqdm(dataset_vectors[\"train\"])]\n",
    "print(\"done1\")\n",
    "data_dicts = [{\"image\": image, \"vector\": vector} for image, vector in tqdm(zip(images, vectors))]\n",
    "print(\"done2\")\n",
    "hf_dataset = Dataset.from_list(data_dicts)\n",
    "print(\"done3\")\n",
    "hf_dataset.push_to_hub(\"norbujam/LG-dataset-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8152efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset single-download(use this or multi download)\n",
    "logging.basicConfig(filename='/download.log', level=logging.INFO,format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "dataset = load_dataset(\"norbujam/LG-dataset-1\")\n",
    "def convert_to_tensors(item):\n",
    "    image_tensor = tf.convert_to_tensor(item['image'])\n",
    "    vector_tensor = tf.convert_to_tensor(item['vector'])\n",
    "    return image_tensor, vector_tensor\n",
    "image_tensors = []\n",
    "vector_tensors = []\n",
    "total_items = len(dataset['train'])\n",
    "pbar = tqdm(total=total_items, desc=\"Converting to TensorFlow tensors\", unit=\"item\")\n",
    "update_counter = 0\n",
    "for idx, item in enumerate(dataset['train']):\n",
    "    image_tensor, vector_tensor = convert_to_tensors(item)\n",
    "    image_tensors.append(image_tensor)\n",
    "    vector_tensors.append(vector_tensor)\n",
    "    pbar.update(1)\n",
    "    update_counter += 1\n",
    "    if update_counter == 1000:\n",
    "        logging.info(f'Converted {idx + 1} items to TensorFlow tensors')\n",
    "        update_counter = 0 \n",
    "pbar.close()\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((image_tensors, vector_tensors))\n",
    "num_elements = tf.data.experimental.cardinality(tf_dataset).numpy()\n",
    "print(f\"Number of elements in the final TensorFlow dataset: {num_elements}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset multi-download\n",
    "logging.basicConfig(filename='/download-training.log', level=logging.INFO,format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "def convert_to_tensors(item):\n",
    "    image_tensor = tf.convert_to_tensor(item['image'])\n",
    "    vector_tensor = tf.convert_to_tensor(item['vector'])\n",
    "    return image_tensor, vector_tensor\n",
    "tf_dataset = None\n",
    "for dataset_number in range(1, 14):\n",
    "    dataset_name = f\"norbujam/LG-dataset-{dataset_number}\"\n",
    "    dataset = load_dataset(dataset_name)['train']\n",
    "    image_tensors = []\n",
    "    vector_tensors = []\n",
    "    total_items = len(dataset)\n",
    "    pbar = tqdm(total=total_items, desc=f\"Downloading and converting {dataset_name}\", unit=\"item\")\n",
    "    update_counter = 0\n",
    "    for idx, item in enumerate(dataset):\n",
    "        image_tensor, vector_tensor = convert_to_tensors(item)\n",
    "        image_tensors.append(image_tensor)\n",
    "        vector_tensors.append(vector_tensor)\n",
    "        pbar.update(1)\n",
    "        update_counter += 1\n",
    "        if update_counter == 1000:\n",
    "            logging.info(f'Converted {len(image_tensors)} items from {dataset_name} to TensorFlow tensors')\n",
    "            update_counter = 0 \n",
    "    pbar.close()\n",
    "    current_tf_dataset = tf.data.Dataset.from_tensor_slices((image_tensors, vector_tensors))\n",
    "    if tf_dataset is None:\n",
    "        tf_dataset = current_tf_dataset\n",
    "    else:\n",
    "        tf_dataset = tf_dataset.concatenate(current_tf_dataset)\n",
    "    logging.info(f'Converted {dataset_number} repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cgan\n",
    "def build_generator(vector_dim, noise_dim, img_shape):\n",
    "    input_vector = layers.Input(shape=(vector_dim,))\n",
    "    input_noise = layers.Input(shape=(noise_dim,))\n",
    "    x = layers.Concatenate()([input_vector, input_noise])\n",
    "    x = layers.Dense(35*700, activation='relu')(x)\n",
    "    x = layers.Reshape((35,700,1))(x)\n",
    "    x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2DTranspose(32, (5, 5), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "    output_img = layers.Conv2D(1, (5, 5), activation='tanh', padding='same')(x)\n",
    "    return Model(inputs=[input_vector, input_noise], outputs=output_img)\n",
    "def build_discriminator(vector_dim, img_shape):\n",
    "    input_vector = layers.Input(shape=(vector_dim,))\n",
    "    input_img = layers.Input(shape=img_shape)\n",
    "    reshaped_img = layers.Reshape((img_shape[0], img_shape[1], 1))(input_img)\n",
    "    x = layers.Conv2D(32, (5, 5), strides=(2, 2), padding='valid', activation='relu')(reshaped_img)\n",
    "    x = layers.Conv2D(16, (5, 5), strides=(3, 3), padding='valid', activation='relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Concatenate()([x, input_vector])\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs=[input_vector, input_img], outputs=output)\n",
    "vector_dim = 768  \n",
    "noise_dim = 100  \n",
    "img_shape = (70, 1400)\n",
    "generator = build_generator(vector_dim, noise_dim, img_shape)\n",
    "discriminator = build_discriminator(vector_dim, img_shape)\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "@tf.function\n",
    "def train_step(images, vectors):\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator([vectors, noise], training=True)\n",
    "        real_output = discriminator([vectors, images], training=True)\n",
    "        fake_output = discriminator([vectors, generated_images], training=True)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "def train(dataset, epochs, checkpoint_path, save_every=100):\n",
    "    total_batches = len(dataset)\n",
    "    for epoch in tqdm(range(epochs), desc='Training', unit='epoch'):\n",
    "        for image_batch, vector_batch in tqdm(dataset):\n",
    "            train_step(image_batch, vector_batch)\n",
    "        logging.info(f'Epoch {epoch + 1}/{epochs} - Batch processed')\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            logging.info(f'Saving model checkpoint at epoch {epoch + 1}')\n",
    "            generator.save(f'{checkpoint_path}/generator_epoch_{epoch + 1}.h5')\n",
    "            discriminator.save(f'{checkpoint_path}/discriminator_epoch_{epoch + 1}.h5')\n",
    "    logging.info(f'Training completed for {epochs} epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f7fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset shaping\n",
    "tf_dataset = tf_dataset.cache()\n",
    "tf_dataset = tf_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "batch_size = 32\n",
    "mydataset=tf_dataset.batch(batch_size, drop_remainder=True)\n",
    "def reshape_image(image, label):\n",
    "    label_reshaped = tf.squeeze(label, axis=1)\n",
    "    return image, label_reshaped\n",
    "mydataset = mydataset.map(reshape_image)\n",
    "checkpoint_path = '/model_checkpoints' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "train(mydataset, epochs=16, checkpoint_path=checkpoint_path, save_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "model_path = 'model_checkpoints/generator_epoch_2.h5'\n",
    "generator = load_model(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openpecha/tibetan_RoBERTa_S_e6\")\n",
    "model = TFAutoModel.from_pretrained(\"openpecha/tibetan_RoBERTa_S_e6\")\n",
    "tibetan_text = \"ཀྱང་ཤེས་རབ་ཀྱི་ཕ་རོལ་ཏུ་ཕྱིན་པ་མ་ཡིན། བྱང་ཆུབ་ཀྱི་ཡན་ལག་རྣམས་ཀྱི་དེ་བཞིན་ཉིད་ལས་གུད་ནའང་ཤེས་རབ་ཀྱི་ཕ་རོལ་ཏུ་ཕྱིན་པ་མེད་དོ། །བྱང་ཆུབ་ཀྱི་ཡན་ལག་རྣམས་\"\n",
    "tokens = tokenizer(tibetan_text, return_tensors=\"tf\")\n",
    "with tf.device('/CPU:0'):\n",
    "    outputs = model(**tokens)\n",
    "embeddings = outputs.last_hidden_state\n",
    "tibetan_vector = embeddings.numpy()\n",
    "normalized_tibetan_vector = tf.keras.utils.normalize(tf.reduce_mean(tibetan_vector, axis=1))\n",
    "noise_dim = 100\n",
    "noise = tf.random.normal([1, noise_dim])\n",
    "input_data = [normalized_tibetan_vector, noise]\n",
    "generated_image = generator(input_data, training=False)\n",
    "generated_image = (generated_image + 1) / 2\n",
    "plt.imshow(generated_image[0])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
